{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "\n",
    "# import models\n",
    "# import train_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(train_lstm)\n",
    "# importlib.reload(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seed = 1289719\n",
    "series_min_len = 144 #12hrs\n",
    "pred_horizon = 12 # 1hr\n",
    "input_size = series_min_len - pred_horizon\n",
    "output_size = pred_horizon\n",
    "import math\n",
    "dataset_path = '../timeseries_0-patients_step-1_len-x.pkl'\n",
    "\n",
    "# group_id = 2\n",
    "# dataset_path = \"timeseries_\"+str(group_id)+\"-patients_step-1.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "save_every = 1\n",
    "test_every = save_every\n",
    "\n",
    "epoch_start = 0\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print('Training on GPU!')\n",
    "else:\n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')\n",
    "\n",
    "# lstm_trainer = train_lstm.LSTM_Trainer(seed, device)\n",
    "\n",
    "# heurisitic = models.Heuristic(1, 1).to(device)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Copy Last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# #load test set\n",
    "# lstm_trainer.load_test(dataset_path, 1, input_size, pred_horizon, 0)\n",
    "# data_loader = lstm_trainer.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# num_batches = len(data_loader)\n",
    "# print(num_batches)\n",
    "# total_loss = 0\n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "# esod = 0\n",
    "# for X, y in data_loader:\n",
    "#     last = X[0][-1]\n",
    "#     prediction_full = last.repeat(1,pred_horizon)\n",
    "#     #MSE\n",
    "#     total_loss += loss_func(prediction_full, y).item()\n",
    "\n",
    "#     #ESOD\n",
    "#     # esod_pred = 0\n",
    "#     # esod_y = 0\n",
    "#     # preds = prediction_full.cpu().detach().numpy()[0]\n",
    "#     # y_t = y.cpu().detach().numpy()[0]\n",
    "#     # for j in range(2, len(preds)):\n",
    "#     #     esod_pred += math.pow((preds[j] - 2*preds[j-1] + preds[j-2]), 2)\n",
    "#     #     esod_y += math.pow((y_t[j] - 2*y_t[j-1] + y_t[j-2]), 2)\n",
    "#     # if (esod_y != 0):\n",
    "#     #     esod += esod_pred / esod_y\n",
    "#     # break\n",
    "\n",
    "# end = time.time()\n",
    "\n",
    "# avg_loss = total_loss / num_batches\n",
    "# print(f\"Test loss: {avg_loss}\")\n",
    "# print(\"avg ESOD:\", esod/num_batches)\n",
    "\n",
    "# print(\"Time: \", end-start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "[_, X_test, _, y_test] = pickle.load(open(dataset_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52329\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "epochs = len(X_test)\n",
    "# epochs = 10000\n",
    "\n",
    "\n",
    "num_batches = len(X_test)\n",
    "print(num_batches)\n",
    "total_loss = 0\n",
    "esod = 0\n",
    "\n",
    "input_size = 5 \n",
    "\n",
    "# array to store the loss values for each hyperparameter combination\n",
    "losses = []\n",
    "min_loss = 99999999999\n",
    "\n",
    "# hyperparameters to try for higher order regression\n",
    "# learning_rates = [0.001, 0.01, 0.1]\n",
    "degrees = [1, 2, 3, 4, 5, 6]\n",
    "last_n_steps = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i in range(0,epochs):\n",
    "    X = X_test[i]\n",
    "    y = y_test[i]\n",
    "\n",
    "    for last_n in last_n_steps:\n",
    "        \n",
    "        # for degree in degrees:\n",
    "            degree = 1\n",
    "\n",
    "\n",
    "            last = X[-last_n:] \n",
    "            input_size = last_n\n",
    "            \n",
    "            # prediction_full = last.repeat(1,pred_horizon)\n",
    "            reg = LinearRegression().fit(np.arange(0,input_size,1).reshape(input_size,1), last)\n",
    "            # m, c = np.linalg.lstsq(range(0,5), last)\n",
    "            # preds = m * np.linspace(6, 6+12, 1) + c\n",
    "            preds = reg.predict(np.arange(input_size,input_size+pred_horizon,1).reshape(pred_horizon,1))\n",
    "\n",
    "\n",
    "            #MSE\n",
    "            prediction_full = torch.Tensor(preds.squeeze())\n",
    "            y_t = torch.Tensor(y)\n",
    "            loss = loss_func(prediction_full.squeeze(), y_t).item()\n",
    "\n",
    "            # get the best hyperparameters\n",
    "            if (loss < min_loss):\n",
    "                min_loss = loss\n",
    "                best_degree = degree\n",
    "                best_last_n = last_n\n",
    "\n",
    "            losses.append(loss)\n",
    "            total_loss += loss\n",
    "\n",
    "            # #ESOD\n",
    "            # esod_pred = 0\n",
    "            # esod_y = 0\n",
    "            # preds2 = preds.squeeze()\n",
    "            # for j in range(2, len(preds2)):\n",
    "            #     esod_pred += math.pow((preds2[j] - 2*preds2[j-1] + preds2[j-2]), 2)\n",
    "            #     esod_y += math.pow((y[j] - 2*y[j-1] + y[j-2]), 2)\n",
    "            # if (esod_y != 0):\n",
    "            #     esod += esod_pred / esod_y\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Min Losses: \", min(losses))\n",
    "print(\"Best Degree: \", best_degree)\n",
    "print(\"Best Last N: \", best_last_n)\n",
    "\n",
    "\n",
    "avg_loss = total_loss / epochs\n",
    "print(f\"Test loss: {avg_loss}\")\n",
    "print(\"avg ESOD:\", esod/num_batches)\n",
    "\n",
    "print(\"Time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting an example of the best hyperparameter combination\n",
    "X = X_test[0]\n",
    "y = y_test[0]\n",
    "\n",
    "degree = best_degree\n",
    "last_n = best_last_n\n",
    "\n",
    "last = X[-last_n:]\n",
    "input_size = last_n\n",
    "\n",
    "reg = LinearRegression().fit(np.arange(0,input_size,1).reshape(input_size,1), last)\n",
    "preds = reg.predict(np.arange(input_size,input_size+pred_horizon,1).reshape(pred_horizon,1))\n",
    "\n",
    "plt.plot(np.arange(0,input_size,1), last, label=\"Input\")\n",
    "plt.plot(np.arange(input_size,input_size+pred_horizon,1), preds, label=\"Prediction\")\n",
    "plt.plot(np.arange(input_size,input_size+pred_horizon,1), y, label=\"Ground Truth\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "* learning rate\n",
    "* number of iterations\n",
    "* last n of samples to use for prediction\n",
    "* order of polynomial (1-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arima imports\n",
    "import pmdarima as pm\n",
    "from pmdarima import model_selection, pipeline ,arima, metrics\n",
    "from pmdarima import preprocessing as ppc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
